---
title: "Logistic Regression Models"
author: "Juan Rivillas"
date: "11/08/2022"
output: html_document
---

This R Markdown includes regression models with categorical predictors (diseases)

#We will cover the following topics:
Properly defining a classification problem 
Applying logistic regression 
GLMs
Model results explained 
Evaluating the model's performance

#Learning outcomes
we will learn to perform classifications through logistic regression techniques.
we learn how to define a classification problem and how to apply logistic regression to resolve this type of problem.
how to use the Generalized Linear Models (GLMs) to perform logistic regression.
use useful tools to evaluate performances, and deeply understand every parameter of it.

At the end, you will be able to perform a logistic regression and be able to apply logistic regression methods to your own data.



In R, to fit GLMs we can use the glm () function.

###Simple logistic regression###
We will look at a simple logistic regression model (one regressor only).

Hypothesis: Recent studies argue that people who were exposed to childhood adverse condictions, may be exposed to the risk of cardiometabolic diseases in adulthood.

To start, we import the values in the table in R. We need to create a double-entry table; let's proceed as follows
Let's look at the table by invoking the hypertension variable
```{r}
tabhta <- xtabs(~hta + aces, data = dataBA2)
tabhta
```


We now organize the data to carry out logistic regression. We need to create a data frame:
```{r}
dfhta <- as.data.frame(tabhta)
dfhta
#We see the result:
```


At this point, we can use the ????? function to build the logistic regression model:
```{r}
LogModel <- glm(hta ~ aces, weights = Freq, data = dfhta, family = binomial(logit))
```

Explanation of this argument:
We now comment on the code used to build the regression model. Logistic regression is called by imposing the family: family = binomial(logit). The hta ~ aces code means that we want to create a model that explains the hta variable (presence or absence of hypertension) depending on the aces variable (presence or absence of childhood adversity events). In practice, aces is the independent variable X, and hta is the dependent variable Y. The weights argument is used to give the number of trials when the response is the proportion of successes, in our case data contained in the Freq column of the dfhta data frame (so we write weights=Freq). Then we read data=dfhta to specify the location where the values are contained.


To produce result summaries of the results of the model fitting function, we use the summary function:
```{r}
summary(LogModel)
#The results are shown in the following:
```


Next, we see the deviance residuals, which are a measure of model fit. This part of the output shows the distribution of the deviance residuals for individual cases used in the model.


The next part of the summary results shows the coefficients, their standard errors, the z-statistic, and the associated p-values.

We can also see that both coefficients are no significant (large p-values than 0.05)
The logistic regression coefficients give the change in the log odds of the outcome for a single unit increase in the predictor variable. So, for every single unit change in ACES, the log odds of hypertension onset (versus non-hypertension onset) increases by 0.03823.

Below the table of coefficients are fit indices, including the null and deviance residuals and the Akaike Information Criterion (AIC). Null deviance indicates the response predicted by a model with nothing but an intercept. The lower the value, the better the model. 


The analogous metric of adjusted R-squared in logistic regression is AIC. AIC is the measure of fit which penalizes the model for the number of model coefficients. Therefore, we always prefer models with minimum AIC value.

We evaluate this equation in R:
```{r}
Px1=(exp(0.150285+0.03823))/(1+exp(0.15028+0.03823))
Px1
#The result is shown in the following:
```

So, the probability of having HTA is equal to 54,6 percent.

To extract the model coefficients we can write:
```{r}
Beta = LogModel$coefficient[1] 
Alpha = LogModel$coefficient[2]

Beta
Alpha
#The results are shown in the following:
```

Calculate the probabilities associated with the two conditions imposed by the regressor:
```{r}
P0 <- exp(LogModel$coefficient[1]) / (1 + exp(LogModel$coefficient[1])) 
P1 <- exp(LogModel$coefficient[1] + LogModel$coefficient[2]) / (1 + exp(LogModel$coefficient[1]+LogModel$coefficient[2]))

P0
P1
#Obtaining the following results:
```

```{r}
#We can then calculate odds
odds0 <- P0 / (1 - P0) 
odds1 <- P1 / (1 - P1)

odds0
odds1
#The results are shown in the following:
```


```{r}
#We can finally calculate the odd ratio: 
OR1 <- odds1 / odds0 

OR1
#Obtaining the following result:
```

Interpretation: 
A person who has experienced a stressful event during childhood has a tendency to develop hypertension 1 times greater than the person who has not undergone stressful events.

```{r}
OR2 <- (dfhta[1,1]*dfhta[2,2]) / (dfhta[1,2]*dfhta[2,1])
```

####Multiple logistic regression####

This is a classification problem with three inputs and one dichotomy output. We will try to solve this classification problem with logistic regression. Let's start by uploading the content data into a .csv file in the R environment:

```{r}
setwd("C:/R")
data=read.csv('RestaurantTips.csv',sep=",",header=TRUE) View(data)
```


This first command sets the working directory where we will have inserted the dataset for the next reading. The second command use the read.cvs () function that reads a file in table format and creates a data frame from it, with cases corresponding to rows and variables to columns in the file. The last command invokes a spreadsheet-style data viewer on a matrix-like R object. Now let's look at the variables in the dataset: 

```{r}
names(data)
```

As anticipated, there are eight predictors (ACES-7 and childhood SEP) and four responses (HTA, CVD, DM, and obesity). Before proceeding with the fitting of the model, let's see if you can extract useful information through a first exploratory investigation. 

```{r}
#The scatter plot matrix for the four variables
plot(dataBA2[,5:13])
```

As can be seen, there are no particular trends. Let us then conduct a correlation analysis using the cor() function:

```{r}
cor(dataBA2[,5:13])
```

Change characters to factor
dataBA2$aces   <- as.factor(as.character(dataBA2$aces))
dataBA2$low_childhood_sep <- as.factor(as.character(dataBA2$low_childhood_sep))
dataBA2$household_violence <- as.factor(as.character(dataBA2$household_violence))
dataBA2$emotional_abuse <- as.factor(as.character(dataBA2$emotional_abuse))
dataBA2$early_infection <- as.factor(as.character(dataBA2$early_infection))
dataBA2$migration_yo2   <- as.factor(as.character(dataBA2$migration_yo2))
dataBA2$neglected_food  <- as.factor(as.character(dataBA2$neglected_food))
dataBA2$poor_health2    <- as.factor(as.character(dataBA2$poor_health2))

dataBA2$obesity    <- as.factor(as.character(dataBA2$obesity))
dataBA2$smoking    <- as.factor(as.character(dataBA2$smoking))
dataBA2$alcohol    <- as.factor(as.character(dataBA2$alcohol))

Now, we build the object of formula type which we will include in the glm() function:
```{r}
formula1=obesity ~ aces + low_childhood_sep + household_violence + emotional_abuse + poor_health2 + early_infection + migration_yo2 + neglected_food
```



Then we can build the model:
```{r}
LGModel<-glm(formula1, data = dataBA2, family = binomial(logit))
summary(LGModel)

#The results are shown in the following:
```

From the analysis of model coefficients, it can be noticed that intercept, low aces risk, low childhood SEP, and household violence Yes variables are statistically significant. Recall that correlation analysis had also shown similar results.

Let's try then to see what happens if we remove from the model, terms that are not statistically significant:

```{r}
LGModel1<-glm(obesity ~ aces + low_childhood_sep + household_violence, data = dataBA2, family = binomial(logit))
summary(LGModel1)

#The results of the new model are shown in the following:
```

In this case, low childhood SEP, ACES and domestic violence terms are statistically significant. At this point we can use the model to make predictions. To do this, we will use the predict() function. 

The function predict() is a generic function for predictions from the results of various model fitting functions. This function invokes particular methods which depend on the class of the first argument:

```{r}
LGModel1Pred <- round(predict(LGModel1, dataBA2, type="response"))
```

In this line of code, we applied the predict () function to the previously built logistic regression model (LGModel1), passing the entire set of data at our disposal. The results were then rounded off using the round() function.

To calculate the confusion matrix to check classification problems in the model 
install.package("caret")
library(lattice)
library(caret)


Now, we can apply the confusionMatriz() function:
This function calculates a cross-tabulation of observed and predicted classes with associated statistics. 
```{r}
LGModel1 <- confusionMatrix(LGModel1Pred, dataBA2[,"obesity"]) 
LGModel1
#The results are shown in the following:
```


Example 2: 
Multiple logistic regression with categorical data.


Use the str function to view a compact display of the structure of an arbitrary R object. In our case, using str (data), we will obtain the following results:
```{r}
str(dataBA2)
```

The dataset is stored in table format with following variables
```{r}
summary(dataBA2)
```


```{r}
taobe <- xtabs(~obesity + aces, data = dataBA2)
taobe
```

#####EMILIE comments#####

show.reflvl = TRUE

ACES and Disease
```{r}
#HTA
log_m1_aces_hta <- glm(hta ~ aces + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_aces_hta <- glm(hta ~ aces + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_aces_hta <- glm(hta ~ aces + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_aces_hta <- glm(hta ~ aces + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

#Diabetes
log_m1_aces_dm <- glm(diabetes ~ aces + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_aces_dm <- glm(diabetes ~ aces + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_aces_dm <- glm(diabetes ~ aces + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_aces_dm <- glm(diabetes ~ aces + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

#CVD
log_m1_aces_cvd <- glm(cvd ~ aces + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_aces_cvd <- glm(cvd ~ aces + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_aces_cvd <- glm(cvd ~ aces + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_aces_cvd <- glm(cvd ~ aces + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

#Obesity
log_m1_aces_o <- glm(obesity ~ aces + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_aces_o <- glm(obesity ~ aces + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_aces_o <- glm(obesity ~ aces + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_aces_o <- glm(obesity ~ aces + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

tab_model(log_m1_aces_hta, log_m2_aces_hta, log_m3_aces_hta, log_m4_aces_hta)
tab_model(log_m1_aces_dm, log_m2_aces_dm, log_m3_aces_dm, log_m4_aces_dm)
tab_model(log_m1_aces_cvd, log_m2_aces_cvd, log_m3_aces_cvd, log_m4_aces_cvd)
tab_model(log_m1_aces_o, log_m2_aces_o, log_m3_aces_o, log_m4_aces_o)

```

SEP and Disease
```{r}
#HTA
log_m1_sep_hta <- glm(hta ~ low_childhood_sep + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_sep_hta <- glm(hta ~ low_childhood_sep + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_sep_hta <- glm(hta ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_sep_hta <- glm(hta ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

#Diabetes
log_m1_sep_dm <- glm(diabetes ~ low_childhood_sep + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_sep_dm <- glm(diabetes ~ low_childhood_sep + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_sep_dm <- glm(diabetes ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_sep_dm <- glm(diabetes ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

#CVD
log_m1_sep_cvd <- glm(cvd ~ low_childhood_sep + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_sep_cvd <- glm(cvd ~ low_childhood_sep + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_sep_cvd <- glm(cvd ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_sep_cvd <- glm(cvd ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

#Obesity
log_m1_sep_o <- glm(obesity ~ low_childhood_sep + gender, data = dataBA2, family = binomial(link = "logit"))
log_m2_sep_o <- glm(obesity ~ low_childhood_sep + gender + ethnic_group, data = dataBA2, family = binomial(link = "logit"))
log_m3_sep_o <- glm(obesity ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education, data = dataBA2, family = binomial(link = "logit"))
log_m4_sep_o <- glm(obesity ~ low_childhood_sep + gender + ethnic_group + low_adult_sep + education + smoking + alcohol, data = dataBA2, family = binomial(link = "logit"))

tab_model(log_m1_sep_hta, log_m2_sep_hta, log_m3_sep_hta, log_m4_sep_hta)
tab_model(log_m1_sep_dm, log_m2_sep_dm, log_m3_sep_dm, log_m4_sep_dm)
tab_model(log_m1_sep_cvd, log_m2_sep_cvd, log_m3_sep_cvd, log_m4_sep_cvd)
tab_model(log_m1_sep_o, log_m2_sep_o, log_m3_sep_o, log_m4_sep_o)

```


